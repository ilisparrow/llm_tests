{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements on Intelligent Article Tracker POC\n",
    "\n",
    "- The objective of the POC is to be able to get feedback from users on the fundamental functionality\n",
    "- Fundamental functionality\n",
    "    - Pick a news website to watch\n",
    "    - Add at least one question OR topic of interest\n",
    "    - Get articles relevant to this topic\n",
    "    - Get summary of relevant articles\n",
    "    - In case of question: get short answer based on article content\n",
    "\n",
    "Optinal improvements\n",
    "- langchain json parser\n",
    "- refactor dev dirs (utils file with functionsfor ipynb and python )\n",
    "- Fixe this limit, in a smarter way\n",
    "- DO URL Check and show message when not valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "from bs4 import BeautifulSoup              # Importing BeautifulSoup for HTML parsing\n",
    "from bs4.element import Comment             # Importing Comment class for extracting comments from HTML\n",
    "import urllib.request                       # Importing urllib.request for making HTTP requests\n",
    "import streamlit as st                      # Importing streamlit for building interactive web apps\n",
    "import os                                   # Importing os for accessing operating system functionalities\n",
    "from dotenv import load_dotenv              # Importing load_dotenv for loading environment variables\n",
    "from langchain.llms import OpenAI            # Importing OpenAI class from langchain.llms module\n",
    "from langchain.prompts import PromptTemplate # Importing PromptTemplate class from langchain.prompts module\n",
    "import json                                 # Importing json module for working with JSON data\n",
    "from dotenv import dotenv_values            # Importing dotenv_values for loading environment variables from .env file\n",
    "from googlesearch import search             # Importing search function from googlesearch module\n",
    "import requests                            # Importing requests module for making HTTP requests\n",
    "import unicodedata\n",
    "import validators\n",
    "\n",
    "## SETUP ENVIRONMENT VARIABLES\n",
    "\n",
    "load_dotenv()\n",
    "env_vars = dotenv_values(\".env\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define system relevant input data for application\n",
    "\n",
    "### USER INPUT HERE\n",
    "HARD_LIMIT_CHAR = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "\n",
    "def tag_visible(element):\n",
    "    excluded_tags = ['a', 'style', 'script', 'head', 'title', 'meta', '[document]']\n",
    "\n",
    "    if element.parent.name in excluded_tags:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.find_all(text=tag_visible)\n",
    "    visible_texts = [t.strip() for t in texts if t.strip()]\n",
    "\n",
    "    return \" \".join(visible_texts)\n",
    "\n",
    "\n",
    "def extract_json_values(input_str):\n",
    "    results = []\n",
    "    while input_str:\n",
    "        try:\n",
    "            value = json.loads(input_str)\n",
    "            input_str = \"\"\n",
    "        except json.decoder.JSONDecodeError as exc:\n",
    "            if str(exc).startswith(\"Expecting value\"):   \n",
    "                input_str = input_str[exc.pos+1:]\n",
    "                continue\n",
    "            elif str(exc).startswith(\"Extra data\"):\n",
    "                value = json.loads(input_str[:exc.pos])\n",
    "                input_str = input_str[exc.pos:]\n",
    "        results.append(value)\n",
    "    return results\n",
    "\n",
    "## Process website and save content to file\n",
    "def process_website(url, output_file_name):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    text_from_webpage = text_from_html(html)\n",
    "    text_from_webpage = text_from_webpage[:HARD_LIMIT_CHAR]\n",
    "\n",
    "    # Logging\n",
    "    file_path = output_file_name\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(text_from_webpage)\n",
    "    print(\"Variable content saved to the file:\", file_path)\n",
    "    return text_from_webpage\n",
    "\n",
    "def get_link_based_on_article_name_via_google(article_title, url_to_watch):\n",
    "    search = article_title + \" \" + url_to_watch\n",
    "    url = 'https://www.google.com/search'\n",
    "\n",
    "    headers = {\n",
    "        'Accept' : '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "    }\n",
    "    parameters = {'q': search}\n",
    "\n",
    "    content = requests.get(url, headers = headers, params = parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    search = soup.find(id = 'search')\n",
    "    first_link = search.find('a')\n",
    "    article_link= first_link['href']\n",
    "    return first_link['href']\n",
    "\n",
    "\n",
    "def prompt_to_llm_response(text_from_webpage, prompt_input):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"webpage\", \"prompt_text\"],\n",
    "        template=\"\\\"{prompt_text}\\\" \\\n",
    "            webpage :  \\\"{webpage}\\\"\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    prompt_to_send = prompt.format(webpage=text_from_webpage, prompt_text=prompt_input)\n",
    "\n",
    "\n",
    "    llm = OpenAI(openai_api_key=env_vars['OPENAI_API_KEY'], temperature=0)\n",
    "    result_from_chatgpt = llm(prompt_to_send).replace(\"\\n\", \"\").replace(\"Answer:\",\"\")\n",
    "    return result_from_chatgpt\n",
    "\n",
    "\n",
    "def prompt_similarity_to_llm_response(sentence1, sentence2):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"sentence1\", \"sentence2\"],\n",
    "        template=\"\"\"\n",
    "            Compare the content of the following two sentences. Could sentence 1 be relevant for a person interested in sentence 2? \n",
    "            Answer with one of [strongly agree, agree, disagree, strongly disagree] only.\n",
    "\n",
    "            Sentence 1: {sentence1}\n",
    "            Sentence 2: {sentence2}\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    prompt_to_send = prompt.format(sentence1=sentence1, sentence2=sentence2)\n",
    "\n",
    "\n",
    "    llm = OpenAI(openai_api_key=env_vars['OPENAI_API_KEY'], temperature=0)\n",
    "    result_from_chatgpt = llm(prompt_to_send).replace(\"\\n\", \"\").replace(\"Answer:\",\"\").lower()\n",
    "    return result_from_chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/dv77d0852z9ft3dtpr8r4bz00000gn/T/ipykernel_40484/272102604.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.find_all(text=tag_visible)\n"
     ]
    }
   ],
   "source": [
    "## Web Scrapping\n",
    "\n",
    "url_input = \"https://news.yahoo.com\"\n",
    "# url_input = \"https://laion.ai/blog/\" # OK\n",
    "# url_input = \"https://www.euronews.com/tag/artificial-intelligence\" # NOK\n",
    "# url_input = \"https://www.theguardian.com/international\" #OK\n",
    "# url_input = \"https://www.bloomberg.com/europe\" #NOK\n",
    "# url_input = \"https://news.google.com/home?hl=en-US&gl=US&ceid=US:en\" # OK\n",
    "\n",
    "### USER INPUT HERE\n",
    "if validators.url(url_input):\n",
    "    url_to_watch = st.text_input(\"Input your URL here\", url_input)\n",
    "    ## Process website and save content to file\n",
    "    text_from_webpage = process_website(url_to_watch, \"output.txt\")\n",
    "    text_from_webpage = text_from_webpage[:HARD_LIMIT_CHAR]\n",
    "else: \n",
    "    print(\"URL not valid\")\n",
    "    ### UI OUTPUT HERE\n",
    "    #st.write(\"URL not valid\")  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_news = \"Below is an html version of a news website. It contains news articles. Find the titles of news articles on this website. Do not make up article titles. List all the article titles and their metadata if it exists like date or author. Limit yourself to the first 5. In JSON format, using these keys \\\"title\\\", \\\"metadata\\\". No Other text.\"\n",
    "\n",
    "result_from_chatgpt = prompt_to_llm_response(text_from_webpage,prompt_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Mounting attacks on the Russian side of the border\",\n",
      "        \"metadata\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Will AI soon be as smart as  or smarter than  humans?\",\n",
      "        \"metadata\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"'Hair' actor Treat Williams dies in motorcycle crash\",\n",
      "        \"metadata\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Body recovered from I-95 collapse wreckage: Officials\",\n",
      "        \"metadata\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Navajo law enforcement teams made contact with several hundred Native Americans from various tribes who are living on the streets in the metro Phoenix area\",\n",
      "        \"metadata\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "result_from_chatgpt_processed = result_from_chatgpt.encode('ascii', 'ignore')\n",
    "\n",
    "print(json.dumps(json.loads(result_from_chatgpt_processed), indent=4))\n",
    "#print(result_from_chatgpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: output_gpt.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"gpt_out.txt\"\n",
    "\n",
    "parsed_articles = json.loads(result_from_chatgpt)\n",
    "#Logging\n",
    "file_path = \"output_gpt.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_from_chatgpt)\n",
    "print(\"Variable content saved to the file:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/dv77d0852z9ft3dtpr8r4bz00000gn/T/ipykernel_56800/272102604.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.find_all(text=tag_visible)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: article_text1.txt\n"
     ]
    }
   ],
   "source": [
    "#with open('final_output.json', 'w') as f:\n",
    "#  print(\"The json file is created\")\n",
    "\n",
    "### USER INPUT HERE\n",
    "#topic_of_interest = \"Should AI be open sourced?\"\n",
    "topic_of_interest = \"Ukraine War\"\n",
    "\n",
    "empty_list = []\n",
    "i = 0\n",
    "\n",
    "for item in json.loads(result_from_chatgpt_processed):\n",
    "    i+=1\n",
    "    output_filename = \"article_text\"+str(i)+\".txt\"\n",
    "\n",
    "    article_title = item['title']\n",
    "    article_link = get_link_based_on_article_name_via_google(article_title, url_to_watch)\n",
    "    \n",
    "    new_item = {\n",
    "        'title': item['title'],\n",
    "        'metadata': item['metadata'],\n",
    "        'link': article_link,\n",
    "    }\n",
    "\n",
    "\n",
    "    relation_exists = prompt_similarity_to_llm_response(article_title,topic_of_interest)\n",
    "        \n",
    "    if relation_exists == \"strongly agree\" or relation_exists ==  \"agree\" :\n",
    "        article_text = process_website(article_link, output_filename)\n",
    "\n",
    "        # Summarize article\n",
    "        prompt_article = \"Summarize the following text in 3 sentences: \"\n",
    "        article_summary = prompt_to_llm_response(article_text,prompt_article)\n",
    "\n",
    "        # Answer the question\n",
    "        prompt_content = \"If user input is a question provide an answer, otherwise summarise content relevant to the input topic. Answer in one sentence\".format(topic_of_interest)\n",
    "        user_question_answer = prompt_to_llm_response(article_text,prompt_content)\n",
    "    \n",
    "        new_item[\"summary\"]=article_summary\n",
    "        new_item[\"answer\"]=user_question_answer\n",
    "        new_item[\"related?\"]=relation_exists\n",
    "        \n",
    "    #else: print(\"not relevant\")\n",
    "        \n",
    "    empty_list.append(new_item)\n",
    "\n",
    "output_json = json.dumps(empty_list, indent=4)\n",
    "\n",
    "### UI OUTPUT HERE\n",
    "with open(\"output.json\", \"w\") as outfile:\n",
    "    outfile.write(output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
