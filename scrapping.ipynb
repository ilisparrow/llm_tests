{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "from bs4 import BeautifulSoup              # Importing BeautifulSoup for HTML parsing\n",
    "from bs4.element import Comment             # Importing Comment class for extracting comments from HTML\n",
    "import urllib.request                       # Importing urllib.request for making HTTP requests\n",
    "import streamlit as st                      # Importing streamlit for building interactive web apps\n",
    "import os                                   # Importing os for accessing operating system functionalities\n",
    "from dotenv import load_dotenv              # Importing load_dotenv for loading environment variables\n",
    "from langchain.llms import OpenAI            # Importing OpenAI class from langchain.llms module\n",
    "from langchain.prompts import PromptTemplate # Importing PromptTemplate class from langchain.prompts module\n",
    "import json                                 # Importing json module for working with JSON data\n",
    "from dotenv import dotenv_values            # Importing dotenv_values for loading environment variables from .env file\n",
    "from googlesearch import search             # Importing search function from googlesearch module\n",
    "import requests                            # Importing requests module for making HTTP requests\n",
    "\n",
    "\n",
    "## SETUP ENVIRONMENT VARIABLES\n",
    "\n",
    "load_dotenv()\n",
    "env_vars = dotenv_values(\".env\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define system relevant input data for application\n",
    "HARD_LIMIT_CHAR = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "\n",
    "def tag_visible(element):\n",
    "    excluded_tags = ['a', 'style', 'script', 'head', 'title', 'meta', '[document]']\n",
    "\n",
    "    if element.parent.name in excluded_tags:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.find_all(text=tag_visible)\n",
    "    visible_texts = [t.strip() for t in texts if t.strip()]\n",
    "\n",
    "    return \" \".join(visible_texts)\n",
    "\n",
    "\n",
    "def extract_json_values(input_str):\n",
    "    results = []\n",
    "    while input_str:\n",
    "        try:\n",
    "            value = json.loads(input_str)\n",
    "            input_str = \"\"\n",
    "        except json.decoder.JSONDecodeError as exc:\n",
    "            if str(exc).startswith(\"Expecting value\"):   \n",
    "                input_str = input_str[exc.pos+1:]\n",
    "                continue\n",
    "            elif str(exc).startswith(\"Extra data\"):\n",
    "                value = json.loads(input_str[:exc.pos])\n",
    "                input_str = input_str[exc.pos:]\n",
    "        results.append(value)\n",
    "    return results\n",
    "\n",
    "## Process website and save content to file\n",
    "def process_website(url, output_file_name):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    text_from_webpage = text_from_html(html)\n",
    "    text_from_webpage = text_from_webpage[:HARD_LIMIT_CHAR]\n",
    "\n",
    "    # Logging\n",
    "    file_path = output_file_name\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(text_from_webpage)\n",
    "    print(\"Variable content saved to the file:\", file_path)\n",
    "    return text_from_webpage\n",
    "\n",
    "def get_link_based_on_article_name_via_google(article_title, url_to_watch):\n",
    "    search = article_title + \" \" + url_to_watch\n",
    "    url = 'https://www.google.com/search'\n",
    "\n",
    "    headers = {\n",
    "        'Accept' : '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "    }\n",
    "    parameters = {'q': search}\n",
    "\n",
    "    content = requests.get(url, headers = headers, params = parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    search = soup.find(id = 'search')\n",
    "    first_link = search.find('a')\n",
    "    article_link= first_link['href']\n",
    "    return first_link['href']\n",
    "\n",
    "\n",
    "def prompt_to_llm_response(text_from_webpage, prompt_input):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"webpage\", \"prompt_text\"],\n",
    "        template=\"\\\"{prompt_text}\\\" \\\n",
    "            webpage :  \\\"{webpage}\\\"\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    prompt_to_send = prompt.format(webpage=text_from_webpage, prompt_text=prompt_input)\n",
    "\n",
    "\n",
    "    llm = OpenAI(openai_api_key=env_vars['OPENAI_API_KEY'], temperature=0)\n",
    "    result_from_chatgpt = llm(prompt_to_send).replace(\"\\n\", \"\").replace(\"Answer:\",\"\")\n",
    "    return result_from_chatgpt\n",
    "\n",
    "\n",
    "def prompt_similarity_to_llm_response(sentence1, sentence2):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"sentence1\", \"sentence2\"],\n",
    "        template=\"\"\"\n",
    "            Compare the content of the following two sentences. Could sentence 1 be relevant for a person interested in sentence 2? \n",
    "            Answer with one of [strongly agree, agree, disagree, strongly disagree] only.\n",
    "\n",
    "            Sentence 1: {sentence1}\n",
    "            Sentence 2: {sentence2}\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    prompt_to_send = prompt.format(sentence1=sentence1, sentence2=sentence2)\n",
    "\n",
    "\n",
    "    llm = OpenAI(openai_api_key=env_vars['OPENAI_API_KEY'], temperature=0)\n",
    "    result_from_chatgpt = llm(prompt_to_send).replace(\"\\n\", \"\").replace(\"Answer:\",\"\").lower()\n",
    "    return result_from_chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/dv77d0852z9ft3dtpr8r4bz00000gn/T/ipykernel_56800/272102604.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.find_all(text=tag_visible)\n"
     ]
    }
   ],
   "source": [
    "## Web Scrapping\n",
    "\n",
    "#url_to_watch = st.text_input(\"Input your url here\",\"https://www.nytimes.com/international/section/politics\")\n",
    "url_to_watch = st.text_input(\"Input your url here\",\"https://laion.ai/blog/\")\n",
    "#url_to_watch = st.text_input(\"Input your url here\",\"https://www.euronews.com/tag/artificial-intelligence\")\n",
    "\n",
    "## Process website and save content to file\n",
    "text_from_webpage = process_website(url_to_watch, \"output.txt\")\n",
    "text_from_webpage = text_from_webpage[:HARD_LIMIT_CHAR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_news = \"Below is an html version of a news website. Find the titles of news articles on this website. List all the article titles and their metadata if it exists like date or author. Limit yourself to the first 3. In JSON format, using these keys \\\"title\\\", \\\"metadata\\\". No Other text.\"\n",
    "result_from_chatgpt = prompt_to_llm_response(text_from_webpage,prompt_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Announcing DataComp: In search of the next generation of multimodal datasets\",\n",
      "        \"metadata\": \"by: Gabriel Ilharco , 27 Apr, 2023\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"A new Paella: Simple & Efficient Text-To-Image generation\",\n",
      "        \"metadata\": \"by: Dominic Rampas and Pablo Pernias , 15 Apr, 2023\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Petition for keeping up the progress tempo on AI research while securing its transparency and safety.\",\n",
      "        \"metadata\": \"by: LAION.ai , 29 Mar, 2023\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json.loads(result_from_chatgpt), indent=4))\n",
    "#print(result_from_chatgpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: output_gpt.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"gpt_out.txt\"\n",
    "\n",
    "parsed_articles = json.loads(result_from_chatgpt)\n",
    "#Logging\n",
    "file_path = \"output_gpt.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_from_chatgpt)\n",
    "print(\"Variable content saved to the file:\", file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOS\n",
    "- get summarized article via google search\n",
    "- allow input questions about article\n",
    "- answer question\n",
    "- filter based on user interests\n",
    "- use langchain json parser\n",
    "- refactor dev dirs (utils file with functionsfor ipynb and python )\n",
    "- TODO : Fixe this limit, in a smarter way\n",
    "- TODO : DO URL Check and show message when not valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def are_they_related(webpage_title, question):\n",
    "#     #prompt_relation = \"Do this title \\\"{}\\\" and the following webpage text have a word in common?\".format(webpage_title)\n",
    "#     result_from_chatgpt = prompt_to_llm_response(question, prompt_relation)\n",
    "#     return True if result_from_chatgpt.lower()==\"yes\" else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m relation_exists \u001b[39m=\u001b[39m prompt_similarity_to_llm_response(article_title,topic_of_interest)\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m relation_exists \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstrongly agree\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m relation_exists \u001b[39m==\u001b[39m  \u001b[39m\"\u001b[39m\u001b[39magree\u001b[39m\u001b[39m\"\u001b[39m :\n\u001b[0;32m---> 26\u001b[0m     article_text \u001b[39m=\u001b[39m process_website(article_link, output_filename)\n\u001b[1;32m     28\u001b[0m     \u001b[39m# Summarize article\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     prompt_article \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSummarize the following text in 3 sentences: \u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m, in \u001b[0;36mprocess_website\u001b[0;34m(url, output_file_name)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_website\u001b[39m(url, output_file_name):\n\u001b[0;32m---> 39\u001b[0m     html \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlopen(url)\u001b[39m.\u001b[39mread()\n\u001b[1;32m     40\u001b[0m     text_from_webpage \u001b[39m=\u001b[39m text_from_html(html)\n\u001b[1;32m     41\u001b[0m     text_from_webpage \u001b[39m=\u001b[39m text_from_webpage[:HARD_LIMIT_CHAR]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/experiments/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/experiments/lib/python3.8/urllib/request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[39mfor\u001b[39;00m processor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_response\u001b[39m.\u001b[39mget(protocol, []):\n\u001b[1;32m    530\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 531\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[1;32m    533\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/experiments/lib/python3.8/urllib/request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[39m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[0;32m--> 640\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[1;32m    641\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[1;32m    643\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/experiments/lib/python3.8/urllib/request.py:569\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[39mif\u001b[39;00m http_err:\n\u001b[1;32m    568\u001b[0m     args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/experiments/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    501\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    503\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/experiments/lib/python3.8/urllib/request.py:649\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 649\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "#with open('final_output.json', 'w') as f:\n",
    "#  print(\"The json file is created\")\n",
    "#topic_of_interest = \"Should AI be open sourced?\"\n",
    "topic_of_interest = \"US Democracy\"\n",
    "\n",
    "empty_list = []\n",
    "i = 0\n",
    "\n",
    "for item in json.loads(result_from_chatgpt):\n",
    "    i+=1\n",
    "    output_filename = \"article_text\"+str(i)+\".txt\"\n",
    "\n",
    "    article_title = item['title']\n",
    "    article_link = get_link_based_on_article_name_via_google(article_title, url_to_watch)\n",
    "    \n",
    "    new_item = {\n",
    "        'title': item['title'],\n",
    "        'metadata': item['metadata'],\n",
    "        'link': article_link,\n",
    "    }\n",
    "\n",
    "\n",
    "    relation_exists = prompt_similarity_to_llm_response(article_title,topic_of_interest)\n",
    "        \n",
    "    if relation_exists == \"strongly agree\" or relation_exists ==  \"agree\" :\n",
    "        article_text = process_website(article_link, output_filename)\n",
    "\n",
    "        # Summarize article\n",
    "        prompt_article = \"Summarize the following text in 3 sentences: \"\n",
    "        article_summary = prompt_to_llm_response(article_text,prompt_article)\n",
    "\n",
    "        # Answer the question\n",
    "        prompt_content = \"If user input is a question provide an answer, otherwise summarise content relevant to the input topic. Answer in one sentence\".format(topic_of_interest)\n",
    "        user_question_answer = prompt_to_llm_response(article_text,prompt_content)\n",
    "    \n",
    "        new_item[\"summary\"]=article_summary\n",
    "        new_item[\"answer\"]=user_question_answer\n",
    "        new_item[\"related?\"]=relation_exists\n",
    "        \n",
    "    #else: print(\"not relevant\")\n",
    "        \n",
    "    empty_list.append(new_item)\n",
    "\n",
    "output_json = json.dumps(empty_list, indent=4)\n",
    "with open(\"output.json\", \"w\") as outfile:\n",
    "    outfile.write(output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
