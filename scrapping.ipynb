{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "from bs4 import BeautifulSoup              # Importing BeautifulSoup for HTML parsing\n",
    "from bs4.element import Comment             # Importing Comment class for extracting comments from HTML\n",
    "import urllib.request                       # Importing urllib.request for making HTTP requests\n",
    "import streamlit as st                      # Importing streamlit for building interactive web apps\n",
    "import os                                   # Importing os for accessing operating system functionalities\n",
    "from dotenv import load_dotenv              # Importing load_dotenv for loading environment variables\n",
    "from langchain.llms import OpenAI            # Importing OpenAI class from langchain.llms module\n",
    "from langchain.prompts import PromptTemplate # Importing PromptTemplate class from langchain.prompts module\n",
    "import json                                 # Importing json module for working with JSON data\n",
    "from dotenv import dotenv_values            # Importing dotenv_values for loading environment variables from .env file\n",
    "from googlesearch import search             # Importing search function from googlesearch module\n",
    "import requests                            # Importing requests module for making HTTP requests\n",
    "\n",
    "\n",
    "## SETUP ENVIRONMENT VARIABLES\n",
    "\n",
    "load_dotenv()\n",
    "env_vars = dotenv_values(\".env\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define system relevant input data for application\n",
    "HARD_LIMIT_CHAR = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "\n",
    "def tag_visible(element):\n",
    "    excluded_tags = ['a', 'style', 'script', 'head', 'title', 'meta', '[document]']\n",
    "\n",
    "    if element.parent.name in excluded_tags:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.find_all(text=tag_visible)\n",
    "    visible_texts = [t.strip() for t in texts if t.strip()]\n",
    "\n",
    "    return \" \".join(visible_texts)\n",
    "\n",
    "\n",
    "def extract_json_values(input_str):\n",
    "    results = []\n",
    "    while input_str:\n",
    "        try:\n",
    "            value = json.loads(input_str)\n",
    "            input_str = \"\"\n",
    "        except json.decoder.JSONDecodeError as exc:\n",
    "            if str(exc).startswith(\"Expecting value\"):   \n",
    "                input_str = input_str[exc.pos+1:]\n",
    "                continue\n",
    "            elif str(exc).startswith(\"Extra data\"):\n",
    "                value = json.loads(input_str[:exc.pos])\n",
    "                input_str = input_str[exc.pos:]\n",
    "        results.append(value)\n",
    "    return results\n",
    "\n",
    "## Process website and save content to file\n",
    "def process_website(url, output_file_name):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    text_from_webpage = text_from_html(html)\n",
    "    text_from_webpage = text_from_webpage[:HARD_LIMIT_CHAR]\n",
    "\n",
    "    # Logging\n",
    "    file_path = output_file_name\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(text_from_webpage)\n",
    "    print(\"Variable content saved to the file:\", file_path)\n",
    "    return text_from_webpage\n",
    "\n",
    "def get_link_based_on_article_name_via_google(article_title, url_to_watch):\n",
    "    search = article_title + \" \" + url_to_watch\n",
    "    url = 'https://www.google.com/search'\n",
    "\n",
    "    headers = {\n",
    "        'Accept' : '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "    }\n",
    "    parameters = {'q': search}\n",
    "\n",
    "    content = requests.get(url, headers = headers, params = parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    search = soup.find(id = 'search')\n",
    "    first_link = search.find('a')\n",
    "    article_link= first_link['href']\n",
    "    return first_link['href']\n",
    "\n",
    "\n",
    "def prompt_to_llm_response(text_from_webpage, prompt_input):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"webpage\", \"prompt_text\"],\n",
    "        template=\"\\\"{prompt_text}\\\" \\\n",
    "            webpage :  \\\"{webpage}\\\"\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    prompt_to_send = prompt.format(webpage=text_from_webpage, prompt_text=prompt_input)\n",
    "\n",
    "\n",
    "    llm = OpenAI(openai_api_key=env_vars['OPENAI_API_KEY'], temperature=0)\n",
    "    result_from_chatgpt = llm(prompt_to_send).replace(\"\\n\", \"\").replace(\"Answer:\",\"\")\n",
    "    return result_from_chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/dv77d0852z9ft3dtpr8r4bz00000gn/T/ipykernel_62281/3679167475.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.find_all(text=tag_visible)\n"
     ]
    }
   ],
   "source": [
    "## Web Scrapping\n",
    "\n",
    "#url_to_watch = st.text_input(\"Input your url here\",\"https://www.nytimes.com/international/section/politics\")\n",
    "url_to_watch = st.text_input(\"Input your url here\",\"https://laion.ai/blog/\")\n",
    "\n",
    "## Process website and save content to file\n",
    "text_from_webpage = process_website(url_to_watch, \"output.txt\")\n",
    "text_from_webpage = text_from_webpage[:HARD_LIMIT_CHAR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####________________\n",
      "\"In this web page, can you find a pattern, list all the article titles and their publication dates. Do not mix the date with the reading time. Limit yourself to the first 3. In Json format, using these keys \"title\", \"date\". No Other text.\"             webpage :  \"LAION Open Menu Close Menu BLOG Welcome to our LAION blog! Here, you will find commentaries, news, and updates on our current research projects and progress in the field of AI research. These blog posts are not meant to be full scientific research papers, but work in progress to encourage further research / discussions on our discord server and the open scientific community. Announcing DataComp: In search of the next generation of multimodal datasets by: Gabriel Ilharco , 27 Apr, 2023 [ Paper ] [ Code ] [ Website ]\n",
      "About a year ago, we released LAION-5B, a billion-scale open-source image-text dataset. Since then, LAION-5B has become a staple in the open-source machine learning ecosystem, powering open-source models like OpenCLIP, OpenFlamingo, and Stable Diffusion. From the begin... A new Paella: Simple & Efficient Text-To-Image generation by: Dominic Rampas and Pablo Pernias , 15 Apr, 2023 Overview.\n",
      "We are releasing a new Paella model which builds on top of our initial paper https://arxiv.org/abs/2211.07292.\n",
      "Paella is a text-to-image model that works in a quantized latent space and learns similarly to MUSE and Diffusion models.\n",
      "Paella is similar to MUSE as it also works on discrete t... Petition for keeping up the progress tempo on AI research while securing its transparency and safety. by: LAION.ai , 29 Mar, 2023 LINK TO OUR PETITION\n",
      "Authors: Christoph Schuhmann, Huu Nguyen, Robert Kaczmarczyk, Jenia Jitsev &amp; LAION community\n",
      "Securing Our Digital Future: Calling for CERN like international organization to transparently coordinate and progress on large-scale AI research and its safety\n",
      "In an era of unparall... Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning by: Anas Awadalla and Irena Gao , 28 Mar, 2023 Overview.\n",
      "We are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs). Check out our GitHub repository and demo to get started!\n",
      "For t... The OIG Dataset by: By Huu Nguyen -  Ontocord.ai, Sameer Suri, Ken Tsui , Shahules786, Together.xyz team, and Christoph Schuhmann - LAION.ai , 10 Mar, 2023 The Open Instruction Generalist (OIG) dataset is a large open source instruction dataset that currently contains ~43M instructions.\n",
      "OIG is one of many chatbot datasets that LAION, along with its volunteers, Ontocord, Together and other members of the open source community, will be releasing and is i... Training Contrastive Captioners by: Giovanni Puccetti, Maciej Kilian, Romain Beaumont , 02 Feb, 2023 We introduce a new model type to OpenClip Contrastive Captioners (CoCa) [1]. This model adds an autoregressive objective (generation) on top of the CLIP contrastive one. The architecture is composed of three parts, the first two are similar to those composing a CLIP model and the third is a text dec... Clip-Retrieval Update: H-14 Index & SLURM Inference by: no usr , 31 Jan, 2023 Today we release a KNN index for LAION-5B that allows for fast queries of the dataset with the open clip ViT-H-14 CLIP model. This means that users can search through billions of samples quickly and easily, making it a powerful tool for various applications such as image and text retrieval, data fil... Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained on LAION-2B by: Mitchell Wortsman , 24 Jan, 2023 We have trained a new ViT-G/14 CLIP model with OpenCLIP which achieves 80.1% zero-shot accuracy on ImageNet and 74.9% zero-shot image retrieval (Recall@5) on MS COCO. As of January 2023, this is the best open source CLIP model.\n",
      "We believe this is interesting because:\n",
      "\n",
      "CLIP models are useful for zero... Collaboration between LAION and the Stable Horde by: Konstantinos Thoukydidis, hlky , 08 Jan, 2023 Author: Konstantinos Thoukydidis, hlky\n",
      "We are happy to announce that LAION will be assisted by the Stable Horde to provide aesthetic ratings for existing datasets and a completely new dataset of Stable Diffusion generations, which will also be rated by their community.\n",
      "We wrote in the past about LAI... Laion coco: 600M synthetic captions from Laion2B-en by: Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, Romain Beaumont , 15 Sep, 2022 Author: Christoph Schuhmann, Andreas Köpf , Theo Coombes, Richard Vencu, Benjamin Trom , Romain Beaumont\n",
      "We present LAION-COCO, the world’s largest dataset of 600M generated high-quality captions for publicly available web-images\n",
      "Laion5B has five billion natural captions. They provide a lot of infor... Laion translated: 3B captions translated to English from laion5B by: Marianna Nezhurina, Romain Beaumont, Richard Vencu and Christoph Schuhmann , 15 Sep, 2022 Author: Marianna Nezhurina Romain Beaumont Richard Vencu Christoph Schuhmann\n",
      "Laion5B dataset was automatically collected from a section of the human web (common crawl). Can models generate different and interesting data compared to what humans write?\n",
      "That’s a question we are interested in investigat... Large scale openCLIP: L/14, H/14 and g/14 trained on LAION-2B by: Romain Beaumont , 15 Sep, 2022 We trained three large CLIP models with OpenCLIP: ViT-L/14, ViT-H/14 and ViT-g/14 (ViT-g/14 was trained only for about a third the epochs compared to the rest). The H/14 model achieves 78.0% zero shot top-1 accuracy on ImageNet and 73.4% on zero-shot image retrieval at Recall@5 on MS COCO. As of Sep... LAION-Aesthetics by: Christoph Schuhmann , 16 Aug, 2022 We present LAION-Aesthetics, several collections of subsets from LAION 5B with high visual quality.\n",
      "\n",
      "To create LAION-Aesthetics we trained several lightweight models that predict the rating people gave when they were asked “How much do you like this image on a scale from 1 to 10?”.\n",
      "LAION-Aesthetics ... LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS by: Romain Beaumont , 31 Mar, 2022 We present a dataset of 5,85 billion CLIP-filtered image-text pairs, 14x bigger than LAION-400M, previously the biggest openly accessible image-text dataset in the world - see also our NeurIPS2022 paper\n",
      "Authors: Christoph Schuhmann, Richard Vencu, Romain Beaumont, Theo Coombes, Cade Gordon, Aarush K... LAION-400-MILLION OPEN DATASET by: Christoph Schuhmann , 20 Aug, 2021 We present LAION-400M: 400M English (image, text) pairs - see also our Data Centric AI NeurIPS Workshop 2021 paper\n",
      "Concept and Content\n",
      "The LAION-400M dataset is entirely openly, freely accessible.\n",
      "WARNING: be aware that this large-scale dataset is non-curated. It was built for research purposes to e...\"\n",
      "[  {    \"title\": \"Announcing DataComp: In search of the next generation of multimodal datasets\",    \"date\": \"27 Apr, 2023\"  },  {    \"title\": \"A new Paella: Simple & Efficient Text-To-Image generation\",    \"date\": \"15 Apr, 2023\"  },  {    \"title\": \"Petition for keeping up the progress tempo on AI research while securing its transparency and safety.\",    \"date\": \"29 Mar, 2023\"  }]\n",
      "#####________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_news = \"In this web page, can you find a pattern, list all the article titles and their publication dates. Do not mix the date with the reading time. Limit yourself to the first 3. In Json format, using these keys \\\"title\\\", \\\"date\\\". No Other text.\"\n",
    "result_from_chatgpt = prompt_to_llm_response(text_from_webpage,prompt_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Announcing DataComp: In search of the next generation of multimodal datasets\",\n",
      "        \"date\": \"27 Apr, 2023\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"A new Paella: Simple & Efficient Text-To-Image generation\",\n",
      "        \"date\": \"15 Apr, 2023\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Petition for keeping up the progress tempo on AI research while securing its transparency and safety.\",\n",
      "        \"date\": \"29 Mar, 2023\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json.loads(result_from_chatgpt), indent=4))\n",
    "#print(result_from_chatgpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: output_gpt.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"gpt_out.txt\"\n",
    "\n",
    "parsed_articles = json.loads(result_from_chatgpt)\n",
    "#Logging\n",
    "file_path = \"output_gpt.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_from_chatgpt)\n",
    "print(\"Variable content saved to the file:\", file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOS\n",
    "- get summarized article via google search\n",
    "- allow input questions about article\n",
    "- answer question\n",
    "- filter based on user interests\n",
    "- use langchain json parser\n",
    "- refactor dev dirs (utils file with functionsfor ipynb and python )\n",
    "- TODO : Fixe this limit, in a smarter way\n",
    "- TODO : DO URL Check and show message when not valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_they_related(webpage_title, question):\n",
    "    #prompt_relation = \"Do this title \\\"{}\\\" and the following webpage text have a word in common?\".format(webpage_title)\n",
    "    result_from_chatgpt = prompt_to_llm_response(question, prompt_relation)\n",
    "    return True if result_from_chatgpt.lower()==\"yes\" else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/dv77d0852z9ft3dtpr8r4bz00000gn/T/ipykernel_62281/2153384024.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.find_all(text=tag_visible)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: article_text1.txt\n",
      "Variable content saved to the file: article_text2.txt\n",
      "Variable content saved to the file: article_text3.txt\n"
     ]
    }
   ],
   "source": [
    "#with open('final_output.json', 'w') as f:\n",
    "#  print(\"The json file is created\")\n",
    "topic_of_interest = \"Should AI be open sourced?\"\n",
    "\n",
    "empty_list = []\n",
    "i = 0\n",
    "\n",
    "for item in json.loads(result_from_chatgpt):\n",
    "    i+=1\n",
    "    output_filename = \"article_text\"+str(i)+\".txt\"\n",
    "\n",
    "    article_title = item['title']\n",
    "    article_link = get_link_based_on_article_name_via_google(article_title, url_to_watch)\n",
    "    \n",
    "    new_item = {\n",
    "        'title': item['title'],\n",
    "        'date': item['date'],\n",
    "        'link': article_link,\n",
    "    }\n",
    "    #relation_exists = are_they_related(article_title,topic_of_interest)\n",
    "    relation_exists = True\n",
    "\n",
    "    if relation_exists:\n",
    "        article_text = process_website(article_link, output_filename)\n",
    "\n",
    "        # Summarize article\n",
    "        prompt_article = \"Summarize the following text in 3 sentences: \"\n",
    "        article_summary = prompt_to_llm_response(article_text,prompt_article)\n",
    "\n",
    "        # Answer the question\n",
    "        prompt_content = \"If user input is a question provide an answer, otherwise summurise content relevant to the input topic. Answer in one sentence\".format(topic_of_interest)\n",
    "        user_question_answer = prompt_to_llm_response(article_text,prompt_content)\n",
    "    \n",
    "        new_item[\"summary\"]=article_summary\n",
    "        new_item[\"answer\"]=user_question_answer\n",
    "        \n",
    "    #else: print(\"not relevant\")\n",
    "        \n",
    "    empty_list.append(new_item)\n",
    "\n",
    "output_json = json.dumps(empty_list, indent=4)\n",
    "with open(\"ouotput.json\", \"w\") as outfile:\n",
    "    outfile.write(output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
