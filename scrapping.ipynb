{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "from bs4 import BeautifulSoup              # Importing BeautifulSoup for HTML parsing\n",
    "from bs4.element import Comment             # Importing Comment class for extracting comments from HTML\n",
    "import urllib.request                       # Importing urllib.request for making HTTP requests\n",
    "import streamlit as st                      # Importing streamlit for building interactive web apps\n",
    "import os                                   # Importing os for accessing operating system functionalities\n",
    "from dotenv import load_dotenv              # Importing load_dotenv for loading environment variables\n",
    "from langchain.llms import OpenAI            # Importing OpenAI class from langchain.llms module\n",
    "from langchain.prompts import PromptTemplate # Importing PromptTemplate class from langchain.prompts module\n",
    "import json                                 # Importing json module for working with JSON data\n",
    "from dotenv import dotenv_values            # Importing dotenv_values for loading environment variables from .env file\n",
    "from googlesearch import search             # Importing search function from googlesearch module\n",
    "import requests                            # Importing requests module for making HTTP requests\n",
    "\n",
    "\n",
    "## SETUP ENVIRONMENT VARIABLES\n",
    "\n",
    "load_dotenv()\n",
    "env_vars = dotenv_values(\".env\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define system relevant input data for application\n",
    "HARD_LIMIT_CHAR = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "\n",
    "def tag_visible(element):\n",
    "    excluded_tags = ['a', 'style', 'script', 'head', 'title', 'meta', '[document]']\n",
    "\n",
    "    if element.parent.name in excluded_tags:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.find_all(text=tag_visible)\n",
    "    visible_texts = [t.strip() for t in texts if t.strip()]\n",
    "\n",
    "    return \" \".join(visible_texts)\n",
    "\n",
    "\n",
    "def extract_json_values(input_str):\n",
    "    results = []\n",
    "    while input_str:\n",
    "        try:\n",
    "            value = json.loads(input_str)\n",
    "            input_str = \"\"\n",
    "        except json.decoder.JSONDecodeError as exc:\n",
    "            if str(exc).startswith(\"Expecting value\"):   \n",
    "                input_str = input_str[exc.pos+1:]\n",
    "                continue\n",
    "            elif str(exc).startswith(\"Extra data\"):\n",
    "                value = json.loads(input_str[:exc.pos])\n",
    "                input_str = input_str[exc.pos:]\n",
    "        results.append(value)\n",
    "    return results\n",
    "\n",
    "## Process website and save content to file\n",
    "def process_website(url, output_file_name):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    text_from_webpage = text_from_html(html)\n",
    "    text_from_webpage = text_from_webpage[:HARD_LIMIT_CHAR]\n",
    "\n",
    "    # Logging\n",
    "    file_path = output_file_name\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(text_from_webpage)\n",
    "    print(\"Variable content saved to the file:\", file_path)\n",
    "    return text_from_webpage\n",
    "\n",
    "def get_link_based_on_article_name_via_google(article_title):\n",
    "    search = article_title\n",
    "    url = 'https://www.google.com/search'\n",
    "\n",
    "    headers = {\n",
    "        'Accept' : '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "    }\n",
    "    parameters = {'q': search}\n",
    "\n",
    "    content = requests.get(url, headers = headers, params = parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    search = soup.find(id = 'search')\n",
    "    first_link = search.find('a')\n",
    "    article_link= first_link['href']\n",
    "    return first_link['href']\n",
    "\n",
    "\n",
    "def prompt_to_llm_response(text_from_webpage, prompt_input):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"webpage\", \"prompt_text\"],\n",
    "        template=\"\\\"{prompt_text}\\\" \\\n",
    "            webpage :  \\\"{webpage}\\\"\",\n",
    "    )\n",
    "    prompt_to_send = prompt.format(webpage=text_from_webpage, prompt_text=prompt_input)\n",
    "\n",
    "    llm = OpenAI(openai_api_key=env_vars['OPENAI_API_KEY'], temperature=0.9)\n",
    "    result_from_chatgpt = llm(prompt_to_send).replace(\"\\n\", \"\").replace(\"Answer:\",\"\")\n",
    "\n",
    "    return result_from_chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/dv77d0852z9ft3dtpr8r4bz00000gn/T/ipykernel_62281/3046763328.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.find_all(text=tag_visible)\n"
     ]
    }
   ],
   "source": [
    "## Web Scrapping\n",
    "\n",
    "#url_to_watch = st.text_input(\"Input your url here\",\"https://www.nytimes.com/international/section/politics\")\n",
    "url_to_watch = st.text_input(\"Input your url here\",\"https://laion.ai/blog/\")\n",
    "\n",
    "## Process website and save content to file\n",
    "text_from_webpage = process_website(url_to_watch, \"output.txt\")\n",
    "text_from_webpage = text_from_webpage[:HARD_LIMIT_CHAR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_news = \"In this web page, can you find a pattern, list all the article titles and their publication dates. Do not mix the date with the reading time. Limit yourself to the first 3. In Json format, using these keys \\\"title\\\", \\\"date\\\". No Other text.\"\n",
    "result_from_chatgpt = prompt_to_llm_response(text_from_webpage,prompt_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Announcing DataComp: In search of the next generation of multimodal datasets\",\n",
      "        \"date\": \"27 Apr, 2023\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"A new Paella: Simple & Efficient Text-To-Image generation\",\n",
      "        \"date\": \"15 Apr, 2023\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Petition for keeping up the progress tempo on AI research while securing its transparency and safety.\",\n",
      "        \"date\": \"29 Mar, 2023\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json.loads(result_from_chatgpt), indent=4))\n",
    "#print(result_from_chatgpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: output_gpt.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"gpt_out.txt\"\n",
    "\n",
    "parsed_articles = json.loads(result_from_chatgpt)\n",
    "#Logging\n",
    "file_path = \"output_gpt.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_from_chatgpt)\n",
    "print(\"Variable content saved to the file:\", file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOS\n",
    "- get summarized article via google search\n",
    "- allow input questions about article\n",
    "- answer question\n",
    "- filter based on user interests\n",
    "- use langchain json parser\n",
    "- refactor dev dirs (utils file with functionsfor ipynb and python )\n",
    "- TODO : Fixe this limit, in a smarter way\n",
    "- TODO : DO URL Check and show message when not valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/dv77d0852z9ft3dtpr8r4bz00000gn/T/ipykernel_62281/3046763328.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.find_all(text=tag_visible)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable content saved to the file: article_text1.txt\n",
      "Variable content saved to the file: article_text2.txt\n",
      "Variable content saved to the file: article_text3.txt\n"
     ]
    }
   ],
   "source": [
    "#with open('final_output.json', 'w') as f:\n",
    "#  print(\"The json file is created\")\n",
    "\n",
    "empty_list = []\n",
    "i = 0\n",
    "\n",
    "for item in json.loads(result_from_chatgpt):\n",
    "    i+=1\n",
    "    output_filename = \"article_text\"+str(i)+\".txt\"\n",
    "\n",
    "    article_title = item['title']\n",
    "    article_link = get_link_based_on_article_name_via_google(article_title)\n",
    "    article_text = process_website(article_link, output_filename)\n",
    "    \n",
    "    # Summarize article\n",
    "    prompt_article = \"Summarize the following text in 3 sentences: \"\n",
    "    article_summary = prompt_to_llm_response(article_text,prompt_article)\n",
    "    \n",
    "    new_item = {\n",
    "        'title': item['title'],\n",
    "        'date': item['date'],\n",
    "        'link': article_link,\n",
    "        'summary': article_summary\n",
    "    }\n",
    "    empty_list.append(new_item)\n",
    "\n",
    "output_json = json.dumps(empty_list, indent=4)\n",
    "with open(\"ouotput.json\", \"w\") as outfile:\n",
    "    outfile.write(output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"title\": \"Announcing DataComp: In search of the next generation of multimodal datasets\",\n",
      "        \"date\": \"27 Apr, 2023\",\n",
      "        \"link\": \"https://laion.ai/blog/datacomp/\",\n",
      "        \"summary\": \" LAION announced a new benchmark called DataComp to encourage studying design decisions for data sets. DataComp features CommonPool, the largest collection of image-text pairs to date with 12.8B samples and DataComp-1B, a 1.4B subset that can be used to outperform compute-matched CLIP models from OpenAI and LAION. DataComp is designed to accommodate participants with various levels of resources and features pre-determined training procedure, compute requirements, and evaluation suite. It invites participants to innovate by proposing new training sets, leaving the training code, hyper-parameters and compute fixed.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"A new Paella: Simple & Efficient Text-To-Image generation\",\n",
      "        \"date\": \"15 Apr, 2023\",\n",
      "        \"link\": \"https://laion.ai/blog/paella/\",\n",
      "        \"summary\": \"Paella is a new text-to-image model that encodes images into a quantized latent space and is optimized with a CrossEntropy loss. The model's training and sampling code is minimalistic and can be understood and implemented in a few lines of code. It draws inspiration from existing approaches such as MaskGIT and eliminates the need for hyperparameters usually required by other models. Paella has the potential to advance the state of the art in text-to-image synthesis and make it more accessible and comprehensible to a broader audience.\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Petition for keeping up the progress tempo on AI research while securing its transparency and safety.\",\n",
      "        \"date\": \"29 Mar, 2023\",\n",
      "        \"link\": \"https://laion.ai/blog/petition/\",\n",
      "        \"summary\": \"LAION is calling for the establishment of an international, open-source supercomputing research facility, similar to CERN, to democratize AI research and access. This will ensure societal betterment and technological independence while protecting democratic principles. Additionally, the facility should feature AI Safety research labs to quickly identify and address potential risks. Signing the petition is a way to join the campaign to ensure AI technologies are accessible to everyone in a responsible way. Together we can build a brighter future through advanced AI technologies.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json.loads(output_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
